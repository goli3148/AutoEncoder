{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ULPHA67Nhf2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import ToTensor, Compose, Pad\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l_6aJyfANhf3"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "# define model hyperparameters\n",
        "LR = 0.001\n",
        "PATIENCE = 2\n",
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 1\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 2\n",
        "EPOCHS = 100\n",
        "SHAPE_BEFORE_FLATTENING = (128, IMAGE_SIZE // 8, IMAGE_SIZE // 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twl5fBeJNhf4",
        "outputId": "d4d9e4e3-8e19-4ce8-ca24-528de1a6b712"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:08<00:00, 3.28MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 199kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.75MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 18.7MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "torch.Size([1, 32, 32])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transforms = Compose([Pad(padding=2), ToTensor()])\n",
        "train_dataset = FashionMNIST(root='./data', train=True, transform=transforms, download=True)\n",
        "test_dataset  = FashionMNIST(root='./data', train=False, transform=transforms, download=True)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "test_dataloader  = DataLoader(test_dataset,  batch_size=512, shuffle=False)\n",
        "\n",
        "print(train_dataset[0][0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bMW3TP2Nhf5",
        "outputId": "f97569b4-a884-410b-903e-f8518aab9482"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "class Sampling(nn.Module):\n",
        "    def forward(self, z_mean, z_log_var):\n",
        "        batch, dim = z_mean.shape\n",
        "        epsilon = torch.distributions.Normal(0, 1).sample((batch, dim)).to(z_mean.device)\n",
        "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size, embedding_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, stride=2, padding=1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc_mean = nn.Linear(128 * (image_size//8) * (image_size//8), embedding_dim)\n",
        "        self.fc_log_var = nn.Linear(128 * (image_size//8) * (image_size//8), embedding_dim)\n",
        "\n",
        "        self.sample = Sampling()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        z_mean = self.fc_mean(x)\n",
        "        z_log_var = self.fc_log_var(x)\n",
        "\n",
        "        z = self.sample(z_mean, z_log_var)\n",
        "        return z_mean, z_log_var, z\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, shape_before_flattening):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, shape_before_flattening[0]*shape_before_flattening[1]*shape_before_flattening[2])\n",
        "        self.reshape = lambda x: x.view(-1, *shape_before_flattening)\n",
        "        self.deconv1 = nn.ConvTranspose2d(\n",
        "            128, 64, 3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "        self.deconv2 = nn.ConvTranspose2d(\n",
        "            64, 32, 3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "        self.deconv3 = nn.ConvTranspose2d(\n",
        "            32, 1, 3, stride=2, padding=1, output_padding=1\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = self.reshape(x)\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = torch.sigmoid(self.deconv3(x))\n",
        "        return x\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x):\n",
        "        z_mean, z_log_var, z = self.encoder(x)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return z_mean, z_log_var, reconstruction\n",
        "\n",
        "\n",
        "\n",
        "def vae_gaussian_kl_loss(mu, logvar):\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "    return KLD.mean()\n",
        "\n",
        "def reconstruction_loss(x_reconstructed, x):\n",
        "    bce_loss = nn.BCELoss()\n",
        "    return bce_loss(x_reconstructed, x)\n",
        "\n",
        "def vae_loss(y_pred, y_true):\n",
        "    mu, logvar, recon_x = y_pred\n",
        "    recon_loss = reconstruction_loss(recon_x, y_true)\n",
        "    kld_loss = vae_gaussian_kl_loss(mu, logvar)\n",
        "    return 500 * recon_loss + kld_loss\n",
        "\n",
        "encoder = Encoder(IMAGE_SIZE, EMBEDDING_DIM).to(device)\n",
        "decoder = Decoder(EMBEDDING_DIM, SHAPE_BEFORE_FLATTENING).to(device)\n",
        "# pass the encoder and decoder to VAE class\n",
        "vae = VAE(encoder, decoder)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(encoder.parameters()) + list(decoder.parameters()), lr=LR\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.1, patience=PATIENCE, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW4af7fCNhf7",
        "outputId": "c80786e5-6da5-4384-a732-b4d54fdd5da7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 | Train Loss: 196.0746\n"
          ]
        }
      ],
      "source": [
        "# initialize the best validation loss as infinity\n",
        "best_val_loss = float(\"inf\")\n",
        "# start training by looping over the number of epochs\n",
        "for epoch in range(EPOCHS):\n",
        "    # set the vae model to train mode\n",
        "    # and move it to CPU/GPU\n",
        "    vae.train()\n",
        "    vae.to(device)\n",
        "    running_loss = 0.0\n",
        "    # loop over the batches of the training dataset\n",
        "    for batch_idx, (data, _) in enumerate(train_dataloader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass through the VAE\n",
        "        pred = vae(data)\n",
        "        # compute the VAE loss\n",
        "        loss = vae_loss(pred, data)\n",
        "        # backward pass and optimizer step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    train_loss = running_loss / len(train_dataloader)\n",
        "    # print training and validation loss at every 20 epochs\n",
        "    if epoch % 20 == 0 or (epoch+1) == EPOCHS:\n",
        "        print(\n",
        "            f\"Epoch {epoch} | Train Loss: {train_loss:.4f}\"\n",
        "        )\n",
        "    # save best vae model weights based on validation loss"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
